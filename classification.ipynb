{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Splitting (copied from initial_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning set length: 22256 \n",
      "test set length: 5564\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27820 entries, 0 to 27819\n",
      "Data columns (total 24 columns):\n",
      "country               27820 non-null object\n",
      "year                  27820 non-null int64\n",
      "male                  27820 non-null int64\n",
      "age                   27820 non-null object\n",
      "suicides_no           27820 non-null int64\n",
      "population            27820 non-null int64\n",
      "suicides/100k pop     27820 non-null float64\n",
      "country-year          27820 non-null object\n",
      "gdp_for_year $        8364 non-null float64\n",
      "gdp_per_capita $      27820 non-null object\n",
      "gdp_per_capita ($)    27820 non-null int64\n",
      "generation            27820 non-null object\n",
      "Generation X          27820 non-null int64\n",
      "Silent                27820 non-null int64\n",
      "Millenials            27820 non-null int64\n",
      "Boomers               27820 non-null int64\n",
      "G.I. Generation       27820 non-null int64\n",
      "Generation Z          27820 non-null int64\n",
      "5-14 years            27820 non-null int64\n",
      "15-24 years           27820 non-null int64\n",
      "25-34 years           27820 non-null int64\n",
      "35-54 years           27820 non-null int64\n",
      "55-74 years           27820 non-null int64\n",
      "75+ years             27820 non-null int64\n",
      "dtypes: float64(2), int64(17), object(5)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"suiciderates.csv\")\n",
    "\n",
    "#Making generations boolean\n",
    "df_split_gen = df.copy()\n",
    "\n",
    "def gen_x_bool(gen):\n",
    "    if gen == \"Generation X\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Generation X\"] = df_split_gen[\"generation\"].map(gen_x_bool)\n",
    "\n",
    "def silent_bool(gen):\n",
    "    if gen == \"Silent\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Silent\"] = df_split_gen[\"generation\"].map(silent_bool)\n",
    "\n",
    "def mil_bool(gen):\n",
    "    if gen == \"Millenials\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Millenials\"] = df_split_gen[\"generation\"].map(mil_bool)\n",
    "\n",
    "def boom_bool(gen):\n",
    "    if gen == \"Boomers\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Boomers\"] = df_split_gen[\"generation\"].map(boom_bool)\n",
    "\n",
    "def g_i_bool(gen):\n",
    "    if gen == \"G.I. Generation\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"G.I. Generation\"] = df_split_gen[\"generation\"].map(g_i_bool)\n",
    "\n",
    "def z_bool(gen):\n",
    "    if gen == \"Generation Z\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Generation Z\"] = df_split_gen[\"generation\"].map(z_bool)\n",
    "\n",
    "# Drop the original generation column\n",
    "drop_gen = df_split_gen.copy()\n",
    "drop_gen.drop(labels=\"generation\", axis='columns', inplace=True)\n",
    "\n",
    "# Changing sex to a boolean\n",
    "df_sex = drop_gen.copy()\n",
    "df_sex[\"sex\"].replace(\"female\", 0,inplace=True)\n",
    "df_sex[\"sex\"].replace(\"male\", 1, inplace=True)\n",
    "\n",
    "# Rename it to male for easier understanding\n",
    "df_sex.rename(columns = {'sex':'male'}, inplace = True)\n",
    "\n",
    "# Changing age to a boolean\n",
    "age_bool = df_sex.copy()\n",
    "\n",
    "def g1_bool(age):\n",
    "    if age == \"5-14 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"5-14 years\"] = age_bool[\"age\"].map(g1_bool)\n",
    "\n",
    "def g2_bool(age):\n",
    "    if age == \"15-24 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"15-24 years\"] = age_bool[\"age\"].map(g2_bool)\n",
    "\n",
    "def g3_bool(age):\n",
    "    if age == \"25-34 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"25-34 years\"] = age_bool[\"age\"].map(g3_bool)\n",
    "\n",
    "def g4_bool(age):\n",
    "    if age == \"35-54 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"35-54 years\"] = age_bool[\"age\"].map(g4_bool)\n",
    "\n",
    "def g5_bool(age):\n",
    "    if age == \"55-74 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"55-74 years\"] = age_bool[\"age\"].map(g5_bool)\n",
    "\n",
    "def g6_bool(age):\n",
    "    if age == \"75+ years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"75+ years\"] = age_bool[\"age\"].map(g6_bool)\n",
    "\n",
    "# Drop HDI for year\n",
    "drop_hdi = age_bool.copy()\n",
    "drop_hdi.drop(labels=\"HDI for year\", axis='columns', inplace=True)\n",
    "\n",
    "# Convert GDP to numerical values\n",
    "gdp_to_num = drop_hdi.copy()\n",
    "# rename the columns due to formatting issues\n",
    "new_cols = gdp_to_num.columns.values\n",
    "new_cols[8] = 'gdp_for_year $'\n",
    "new_cols[9] = 'gdp_per_capita $'\n",
    "gdp_to_num.columns = new_cols\n",
    "\n",
    "#Change the datatype of gdp_for_year\n",
    "gdp_to_num['gdp_for_year $'] = gdp_to_num.iloc[:,8].str.replace(',', '').astype(float)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data2 = gdp_to_num.copy()\n",
    "train_set, test_set = train_test_split(data2, test_size=0.2, random_state=123)\n",
    "print(\"traning set length:\", len(train_set),\"\\ntest set length:\", len(test_set))\n",
    "print(data2.info())'''\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"suiciderates.csv\")\n",
    "\n",
    "#Making generations boolean\n",
    "df_split_gen = df.copy()\n",
    "\n",
    "def gen_x_bool(gen):\n",
    "    if gen == \"Generation X\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Generation X\"] = df_split_gen[\"generation\"].map(gen_x_bool)\n",
    "\n",
    "def silent_bool(gen):\n",
    "    if gen == \"Silent\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Silent\"] = df_split_gen[\"generation\"].map(silent_bool)\n",
    "\n",
    "def mil_bool(gen):\n",
    "    if gen == \"Millenials\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Millenials\"] = df_split_gen[\"generation\"].map(mil_bool)\n",
    "\n",
    "def boom_bool(gen):\n",
    "    if gen == \"Boomers\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Boomers\"] = df_split_gen[\"generation\"].map(boom_bool)\n",
    "\n",
    "def g_i_bool(gen):\n",
    "    if gen == \"G.I. Generation\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"G.I. Generation\"] = df_split_gen[\"generation\"].map(g_i_bool)\n",
    "\n",
    "def z_bool(gen):\n",
    "    if gen == \"Generation Z\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_split_gen[\"Generation Z\"] = df_split_gen[\"generation\"].map(z_bool)\n",
    "\n",
    "\n",
    "# Changing sex to a boolean\n",
    "df_sex = df_split_gen.copy()\n",
    "df_sex[\"sex\"].replace(\"female\", 0,inplace=True)\n",
    "df_sex[\"sex\"].replace(\"male\", 1, inplace=True)\n",
    "\n",
    "# Rename it to male for easier understanding\n",
    "df_sex.rename(columns = {'sex':'male'}, inplace = True)\n",
    "\n",
    "# Changing age to a boolean\n",
    "age_bool = df_sex.copy()\n",
    "\n",
    "def g1_bool(age):\n",
    "    if age == \"5-14 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"5-14 years\"] = age_bool[\"age\"].map(g1_bool)\n",
    "\n",
    "def g2_bool(age):\n",
    "    if age == \"15-24 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"15-24 years\"] = age_bool[\"age\"].map(g2_bool)\n",
    "\n",
    "def g3_bool(age):\n",
    "    if age == \"25-34 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"25-34 years\"] = age_bool[\"age\"].map(g3_bool)\n",
    "\n",
    "def g4_bool(age):\n",
    "    if age == \"35-54 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"35-54 years\"] = age_bool[\"age\"].map(g4_bool)\n",
    "\n",
    "def g5_bool(age):\n",
    "    if age == \"55-74 years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"55-74 years\"] = age_bool[\"age\"].map(g5_bool)\n",
    "\n",
    "def g6_bool(age):\n",
    "    if age == \"75+ years\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "age_bool[\"75+ years\"] = age_bool[\"age\"].map(g6_bool)\n",
    "\n",
    "\n",
    "# Convert GDP to numerical values\n",
    "gdp_to_num = age_bool.copy()\n",
    "# rename the columns due to formatting issues\n",
    "new_cols = gdp_to_num.columns.values\n",
    "new_cols[8] = 'gdp_for_year $'\n",
    "new_cols[9] = 'gdp_per_capita $'\n",
    "gdp_to_num.columns = new_cols\n",
    "\n",
    "#Change the datatype of gdp_for_year\n",
    "#gdp_to_num['gdp_for_year $'] = gdp_to_num.iloc[:,8].str.replace(',', '').astype(float)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data2 = gdp_to_num.copy()\n",
    "train_set, test_set = train_test_split(data2, test_size=0.2, random_state=123)\n",
    "print(\"traning set length:\", len(train_set),\"\\ntest set length:\", len(test_set))\n",
    "print(data2.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Features:\n",
    "- I am going to choose population for X because that will be the best estimator to distinguish between countries. I will choose suicide_no for Y again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = train_set[[\"population\"]]\n",
    "y = train_set[\"suicides_no\"]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1552  532  232 1022  155  216]\n",
      " [ 938 1174  248  943  160  215]\n",
      " [ 828  539 1137  810  210  190]\n",
      " [ 471  295   22 2780   25  117]\n",
      " [ 821  544  353 1033  783  170]\n",
      " [ 781  523  256 1540  142  499]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The confusion matrix is too large to tell for certain, but it seems like this is a decent confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Sensitivity, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.35608375269590226\n",
      "Precision:  0.3911442799823402\n",
      "Sensitivity:  0.35608375269590226\n",
      "F1:  0.334960421566664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print (\"Accuracy: \", accuracy_score(y, y_predicted))\n",
    "print (\"Precision: \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity: \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1: \", f1_score(y, y_predicted, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- These are all really good values, but it makes me skeptical that there could be over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracies are:  [0.10242587601078167, 0.10604358571107617, 0.09683217254549539, 0.09862952145585262, 0.09638283531790609]\n",
      "Cross validation f1 scores  are:  [0.09772587965647832, 0.10391650511121436, 0.09241915286671885, 0.09418498795039965, 0.09388090523114796]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "validation_accuracy = []\n",
    "validation_f1 =[]\n",
    "fold_and_validate = KFold(n_splits=5, shuffle=True, random_state=145)\n",
    "for train_set_indices, validation_set_indices in fold_and_validate.split(X):\n",
    "    cv_train_set = X.iloc[train_set_indices]\n",
    "    cv_train_target = y.iloc[train_set_indices]\n",
    "    \n",
    "    cv_decision_tree = DecisionTreeClassifier()\n",
    "    cv_decision_tree.fit(cv_train_set, cv_train_target)\n",
    "    \n",
    "    cv_xvalidation = X.iloc[validation_set_indices]\n",
    "    cv_y_true = y.iloc[validation_set_indices]\n",
    "    cv_y_predicted = cv_decision_tree.predict(cv_xvalidation)\n",
    "    \n",
    "    cv_accuracy_score = accuracy_score(cv_y_true, cv_y_predicted)\n",
    "    cv_f1_score = f1_score(cv_y_true, cv_y_predicted,  average=\"weighted\")\n",
    "    validation_accuracy.append(cv_accuracy_score)\n",
    "    validation_f1.append(cv_f1_score)\n",
    "    \n",
    "print(\"Cross validation accuracies are: \", validation_accuracy)\n",
    "print(\"Cross validation f1 scores  are: \", validation_f1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN: K-Nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help from: https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/\n",
    "# importing necessary libraries \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "X = train_set[[\"population\"]]\n",
    "y = train_set[\"suicides_no\"]\n",
    "\n",
    "# dividing X, y into train and test data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123) \n",
    "  \n",
    "# training a KNN classifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[694  54  29 ...   0   0   0]\n",
      " [204  32  28 ...   0   0   0]\n",
      " [121  32  20 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "knn_predicted = knn.predict(X_test)\n",
    "matrix = confusion_matrix(y_test, knn_predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.1419841840402588\n",
      "Precision:  0.06653485304275338\n",
      "Sensitivity:  0.1419841840402588\n",
      "F1:  0.08959923481091099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print (\"Accuracy: \", accuracy_score(y_test, knn_predicted))\n",
    "print (\"Precision: \", precision_score(y_test, knn_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity: \", recall_score(y_test, knn_predicted, average=\"weighted\"))\n",
    "print (\"F1: \", f1_score(y_test, knn_predicted, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- This classifier was much worse. None of the tests came out well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "X = train_set[[\"population\"]]\n",
    "y = train_set[\"suicides_no\"]\n",
    "  \n",
    "# dividing X, y into train and test data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123) \n",
    "  \n",
    "# training a Naive Bayes classifier \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB().fit(X_train, y_train) \n",
    "gnb_predicted = gnb.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[856   0   0 ...   0   0   0]\n",
      " [303   0   2 ...   0   0   0]\n",
      " [222   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test, gnb_predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.1419841840402588\n",
      "Precision:  0.06653485304275338\n",
      "Sensitivity:  0.1419841840402588\n",
      "F1:  0.08959923481091099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print (\"Accuracy: \", accuracy_score(y_test, knn_predicted))\n",
    "print (\"Precision: \", precision_score(y_test, knn_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity: \", recall_score(y_test, knn_predicted, average=\"weighted\"))\n",
    "print (\"F1: \", f1_score(y_test, knn_predicted, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- This classifier was also not great. The tests were all very low and the matrix was not good. The original decision tree was the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = test_set[[\"population\"]]\n",
    "y = test_set[\"suicides_no\"]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[851   0   0 ...   0   0   0]\n",
      " [ 10 310   0 ...   0   0   0]\n",
      " [  4   2 210 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   1   0   0]\n",
      " [  0   0   0 ...   0   1   0]\n",
      " [  0   0   0 ...   0   0   1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9849029475197699\n",
      "Precision:  0.9848352094930028\n",
      "Sensitivity:  0.9849029475197699\n",
      "F1:  0.9845516220486507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\s523286\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print (\"Accuracy: \", accuracy_score(y, y_predicted))\n",
    "print (\"Precision: \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity: \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1: \", f1_score(y, y_predicted, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- The test set was even better than the training set. This was a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Different Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = train_set[[\"suicides_no\"]]\n",
    "y = train_set[\"age\"]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(train_set[\"suicides_no\"], train_set[\"age\"], color=\"red\")\n",
    "plt.xlabel(\"suicides num)\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1552  532  232 1022  155  216]\n",
      " [ 938 1174  248  943  160  215]\n",
      " [ 828  539 1137  810  210  190]\n",
      " [ 471  295   22 2780   25  117]\n",
      " [ 821  544  353 1033  783  170]\n",
      " [ 781  523  256 1540  142  499]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predicted = tree_classifier.predict(X)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
